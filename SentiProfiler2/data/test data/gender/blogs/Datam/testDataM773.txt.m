You may remember the problems that we've been having with our persistent cache, memcacheDB . Our initial response was to add a bunch more RAM to the system, which would probably only last us a few weeks, until we could put a better solution in place. Fortunately with EC2 we were able to spin up five new machines with gobs of RAM to fill that temporary role. After that, we dedicated 33% of our development team (i.e. me) to swapping it out for a more scalable and long-term backend. As of this morning we're now running with our persistent cache backed by Cassandra . The migration was seamless (did you notice?), but the load-impact on our servers is palpable. In case you are wondering why we chose Cassandra: it is way faster, more scalable, and has a rich and active development community full of extremely smart and helpful people. It's in use or going to be in use by several large companies ( Twitter , Facebook , digg , Rackspace). We have the ability to add nodes as load and storage requires, and we have the ability to move non-cache type data into it as appropriate. We could not have done this in just 10 days without the help of the amazing Cassandra developers and community and EC2, which allowed us to bring up new instances on which to test and ultimately deploy Cassandra. I cannot thank the extremely intelligent Cassandra developers and community enough for their work and their help! Photo credit: http://en.wikipedia.org/wiki/File:Solomon_Ajax_and_Cassandra.jpg A - Normal daily reddit traffic B - Yesterday's actual traffic C - What it felt like to us As some of you may have noticed , reddit has been a tad slow lately. Last night, we put in a fix that will hopefully fix the problem in the short term until we can get a longer term fix in place. We wanted to take this opportunity to fill you in on what caused the problem, how we are fixing it, and dispel some myths. In short, we made a technical decision a few years ago that was a good idea at the time, but which is becoming increasingly hard to scale. We need to make some deep changes to fix it, but we put in a band-aid solution of adding more memory for reddit to use in the meantime. But before we get too far down the technical rabbit hole, while the four of us were busy trying to stop the site from melting any further, other things were happening that we didn't get a chance to address. At the end of the day, reddit is both a community and social news site, bound to attract people in the social news business. We have always been about serving up interesting stories and content, all the while trying to ensure that we curb any abuse of the community's good graces. If you like what you see on reddit, good, upvote it. If not, complain , or even make your own community . Above all, if you think someone is abusing the site, tell us . A witch hunt and a glut of personal details degrades us all. Posting personal information crosses the line, and it has been our policy since the beginning to remove it when we see it or when it is pointed out to us. That said, we are not all-seeing. We don't have a program that detects personal information and notifies us. While we removed personal info (per our terms of service) when it was shown to us, we obviously didn't get it all. What happened this weekend saddened us. Saydrah's postings have been additive to the community, and we have no indication that she's been anything but a great moderator to the communities she moderates. Moderators are not exempt from our anti-cheating measures, and, though I hate to have to put it in these terms, we've "investigated" Saydrah, and we didn't find any indication of her cheating or otherwise abusing power. Nerd talk starts here TL;DR: oh hi i upgraded your RAM Myths : The recent site problems are because reddit moved to EC2 -- False The only reason we were able to fix this at all is because we are on EC2 and were able to quickly spin up new instances. In fact, we just had to do it again this morning because we needed even more RAM. But I swear it's been slow since you moved to EC2! -- False We moved to EC2 in May 2009. We only started getting reports of slowness about three weeks ago, which we could also see in our logs and monitoring. The number of reports got worse as time went on, but we didn't see it ourselves, so it was hard to track down a specific cause. reddit is just text, so you guys are clearly morons -- False While reddit is just text when it leaves our servers, there is a whole lot going on under the hood. It is a highly customized user experience, on par with something like Facebook (just not as many users). We do 100's of transactions every second. While we may be doing some things wrong, it's a lot more than just 'select - from comments where article = "foo"' Let's start at the beginning. Here is a simplified version of reddit's architecture: The area where we are having trouble right now is that purple section in the middle that says "memcaches". Specifically, we are having problems with memcachedb, which is where we store a bunch of precomputed listings, like all the listing pages, profile pages, inboxes; pretty much any list on reddit that is too expensive to calculate on the fly. A few years ago, we decided to md5 all of our cache keys. We did this because at the time memcached (which is what memcachedb is based on) could only take keys of a certain length. In fact, the version it is based on still has this limitation. MD5ing the keys was a good solution to this problem, so we thought. It turns out that this one little decision makes it so that we can't horizontally scale that layer of our architecture without losing all the data already there (because all of the keys would point to the wrong server if we added a new one). While we could recalculate all the data, it would take weeks to do so. And that is in fact what we will be doing. Memcachedb has served us well, but it is getting old. It can no longer return data fast enough for our needs, due to the way it interacts with BDb (its underlying data store). We will soon be picking a new data store to replace memcachedb and recalculating all of the data for it, most likely by adding the new cache into the cache chain, and then recalculating what is left after a few weeks. But in the mean time, what we have done is spin up 5 new instances to run memcached. This allowed us to expand the size of the memcache in front of memcachedb to 6GB (up from 2GB). Right now, this means that about 94% of each database is in RAM. As our site grows, this fix will fail us, but hopefully it buys us enough time until we can replace the data store. As usual, if you want to tell us why we suck at our jobs, feel free to leave some comments on this post. One of the main problems with our request to "message a moderator" when you have a problem on a given reddit has been that it is completely opaque as to which moderator to message. Much lore has been generated on many reddits as to who is the "good" or the "responsive" moderator, and many messages have fallen to the wayside by choosing unwisely. By the same token, it is unfair to the moderators that they are unable to see over what may very well be community-wide issues. To mitigate both of these, we've created a separate moderator inbox for each reddit which can be messaged by composing a PM to "# redditname " (with the hashmark stolen from IRC parlance). So, for now on, to reach the admin's box, send a message to "#reddit.com" and we'll get back to you. For other reddits, we've added a handy link to the moderator box. From a moderator's viewpoint, we've make a UI tweak to differentiate your moderator inbox from your regular inbox: And a new orangered alien for when you have moderator mail: Clicking on the alien will bring you to your moderation inbox. In this view, you'll see the messages to all of the reddits you moderate organized with the newest on top. Clicking on the name of the reddit (in the blue bubble on the top left) will bring you to a view that shows messages for that reddit alone. This view is also accessible from the "moderator toolbox" on each of your reddits. One thing to note in responding to these messages: when you participate on a message thread initiated by a user, everything in the conversation will be sent back to the user as an implicit CC. tldr : Don't message one moderator, message all moderators. In pursuit of making reddit easier to use, we've added the ability to get private RSS feeds to parts of the site that would normally require you to log in. The new set of feeds is available thru your preference page , and is available (currently) in both RSS and JSON. [If you are worried about the security of this, we've added a preference to disable it, and set it up so that all feeds will be invalidated once you change your password.] The first example of its use is the front page: we want you to be able to customize your subscriptions to different reddits, but up until now there was no way to get a feed of your personalized front page without passing us your login cookie. This is sometimes handled gracefully by browsers, but seldom by other feed readers. By the same method, we're also generating RSS links for your saved, liked, disliked and hidden pages, as well as to get the contents of your inbox (even your unread tab). And for the moderators in the audience: you'll now find that both the spam and reported listings of your reddits have RSS available. In this case, though, you'll find the feed on the page in question rather than on your preference page. This should make it much easier to keep dibs on what is making its way onto your reddit's spam pages. We've also fixed a bug on those listings that was breaking hiding of links, so moderators will now have the capacity (at least individually) to clear out their spam listing. So, about 8 hours after we put up new markdown rendering engine , our call to find exploits was a complete success, and here is the new white-hat who found it. Besides discovering a bug in the way that discount dealt with escaped quotes in certain places (with thanks to David Parsons of Discount for getting out a patch so quickly), we learned that we should probably set up some guidelines for how to report exploits to us. So, we created a wiki page on help . For those of you who don't know the details of how the infamous worm spread on reddit 5 months ago, here's the short story: a bunch of white-hats created a reddit where they tried to create a proof-of-principle worm that would append itself as a reply to all of the comments in a given page. The whole thing was contained to a shared comment thread, and all of the testing was contained to that thread...until one of the users went to their inbox. You see, the replying JS is common in both areas, so an exploit that works on a comment thread works in your inbox. Your inbox, though, has, in principle, comments from all sorts of other comment threads from all over the site. The result was that the worm got out and was allowed to spread freely until we took down (and later patched) the markdown renderer. The lesson and tldr: please don't post or test for exploits in any shared or public areas. PM us and then make sure your work is well hidden. Once the genie is out, it's hard for us to re-cork it. As most of you know, reddit's commenting system uses a technology called Markdown that allows you to type stuff like: My [site](http://something.com) is -really- cool. ...and have it rendered as: My site is really cool. We get so many comments (and comment-viewings) per second that doing this translation has always been a major burden on our servers. The more CPU cycles we spend on it, the more application servers we need, and in turn the more connections going to our database servers, the more places we have to roll code out to, and all sorts of other growth problems. But you shouldn't feel one ten-thousandth of a guilt unit over this news, because we've just switched our rendering engine from markdown.py (which is nothing to sneeze at and has served us well since the beginning) to a 100%-compatible, pure-C implementation of Markdown known as Discount . (Get it?) In our tests, Discount is fifteen times faster than markdown.py, which should allow us to reclaim some of our computing power and devote it to other areas of the site that are struggling. If you happen to be the sort of person who can't resist looking for security holes in any new technology, and you come across one here, please let us know quietly, rather than choosing to announce your finding in the form of a worm which takes out the entire site . DO NOT SHARE YOUR EXPLOIT WITH ANYONE ELSE. I promise that we'll immediately get to work on the problem as soon as you tell us, and once it's fixed, we'll give you full credit ... as well as the special new ultra-rare "White Hat" trophy. We're also retroactively giving it to previous responsible disclosers chromakode and notrael . The aptly-named javascriptinjection , who wrote and accidentally released the aforementioned worm, will get one too, but only because he immediately got in touch with us as soon as he realized what he had done, then later found a second vulnerability and reported that one the nice way. Hats off to our winners! tl;dr: If you see any wonky comments, tell us . P.S. Special thanks to David Parsons, who is the author and maintainer of Discount and has just been an all-around great guy with respect to this big change. He patiently and quickly answered my endless stream of questions about his code, and even added features to the library that we would have otherwise had to hack in ourselves as a patch. Check out his website , too; it includes photos of his casemod projects and detailed visual archives of some amateur experiments in the field of recombinant DNA research. It was a hell of a day for reddit's Engineering team. 25% of the department (i.e., Chris ) was in New York on Conde Nast business -- I imagine him smoking cigars with bigwigs while everyone's riding polo horses. The rest of us ( David , Jeremy , and me) were preparing for a road trip to the Googleplex to videotape the Peter Norvig AMA . Right when our trio was getting ready to hit the road, news broke that some reddit users were seeing a pop-up ad right smack in the middle of our site. This was a surprise to us, and wasn't even supposed to be possible, but we had an appointment to get to, so we pulled Chris away from, I presume, the squash court to handle things. I'll pass the mic to him to explain how it went: So, honestly, the first thing I thought when I saw that post on the front page was, "oh &#-% . I'm going to have to bludgeon myself to death with my own keyboard, because if I don't someone is going to remind me that I said I would " About half a second later I thought, "well, I'm doomed, but I can at least save the site from further abuse," so I shut off all site-wide ads post haste and hoped that this would give us a chance to figure out what had happened. Soon after, we figured out that this was what we will refer to as a "whoopsie" on the part of our ad systems, and that we had accidentally started running ads from another of our parent company's sites (yes, that's what advertising looks like on other parts of the internet. It is considered "normal". Please buy a T-shirt ...!) The issue was fixed before we had noticed it, but the damage was done. We've decided internally that the best possible fix for this will be to run the ad unit from an &lt;iframe&gt; on a separate domain which will prevent it from busting out by browser security rules (kind of like what we do currently on video embeds). There was nothing malicious involved. This was, apparently, a "feature" of that ad campaign, though it was not intended for us. This was all done in the bit of time I had between meetings. [Oh, and before I forget, don't believe what your manservant tells you: polo is overrated.] Mike here again. Meanwhile, back in Mountain View, the rest of the team had met up with Peter and, after a terrific lunch on Google's dime, grabbed a conference room and began the interview. As Murphy's Law would have it, the site starting acting funny right around the time Jeremy pressed Record. We noticed that our silenced phones seemed to be vibrating a lot more than usual, and so David an
 I quietly checked things out as best we could while Jeremy ran the show. (When we post the video, you may notice the sound of some frantic off-camera typing.) As soon as the taping was finished, we started discussing the anomaly. Within a minute or two of us flipping our ringers back on, all of our phones started simultaneously making the "reddit is in -really- big trouble" noise. The site was down; we didn't even have time to give our hosts a proper goodbye, but instead asked if we could sit in the parking lot and mooch some more WiFi. Google's hospitality, however, was exemplary: they invited us to sit on couches in their lobby for as long as we wanted and help ourselves to all the WiFi we needed. We set up an emergency base of operations next to the reception desk and got Chris on the horn as well. David and Jeremy had brought their laptops along, but mine was 45 minutes away back at reddit HQ, so I had to commandeer one from a generous passerby. (You know who you are. Thanks!) After an hour of rushed coast-to-coast debugging, we had patched things up well enough to survive the trip home. It had been an exciting day, and the Googleplex is every bit the nerd utopia it's reported to be. It's the sort of place where one can leave a laptop sitting on the floor unattended and know that not only is it perfectly safe, but that nobody's going to bat an eyelash over it. For more information, see: Our official statement on pop-over ads Your official statement on pop-over ads (warning: dirty words) Chris's nerdsplanation of the technical difficulties --> If you've visited reddit at all since the earthquake that rocked Haiti two weeks ago, you've undoubtedly noticed the outpouring of support from the reddit community . In fact, we've convinced digg to compete with reddit in fundraising for DirectRelief , which means great things for the people of Haiti; finally a chance for us to use this community rivalry for some good. That said, they've got some catching up to do. Our fundraiser for DirectRelief is nearing $180,000 -- a remarkable feat -- and our non-profit partners took some time to answer your top 10 questions from a recent AMA interview . Here are their answers. Links mentioned in the Q&amp;A: UN Logistics Cluster for the Haiti response http://www.logcluster.org/ops/hti10a Official updates from all UN clusters http://reliefweb.int/rw/rwb.nsf/doc106?OpenForm&amp;view=rwlusppublished&amp;po=72&amp;cc=hti&amp;so=16&amp;stc=or(UNO)&amp;offset=0&amp;hits=50&amp;sortby=rwpubdate&amp;sortdirection=descending